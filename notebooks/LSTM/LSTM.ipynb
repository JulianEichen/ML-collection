{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources**\\\n",
    "$\\rightarrow$ <https://colah.github.io/posts/2015-08-Understanding-LSTMs/> \\\n",
    "$\\rightarrow$ <https://www.quora.com/Does-the-input-dimension-have-to-match-the-cell-state-or-the-hidden-state-dimension-in-an-LSTM> \\\n",
    "$\\rightarrow$ <https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - Long Short Term Memory Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Long Short Term Memory** networks or \"LSTMs\" are a kind of RNN, designed to solve the long-term dependency problem. They were introduced by Hochreiter & Schmidhuber (1997)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a single neural network layer, they consist of 4 special layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LSTM3-chain.png\">\n",
    "<img src=\"LSTM2-notation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Cell State and Gates\n",
    "\n",
    "The cell state (CS) acts as the memory of the LSTM. As the network proceeds to work on different components of the input series, the information can be added to the cell state or removed from it. Thus the LSTM has the ability to control the passed on information.\\\n",
    "Removal and addition of information happens at so called gates. Gates are neural networks that decide which information should be passed on through the cell state. The sigmoid function is a suitable activation function, because it maps to values ranging from 0 to 1. So information can be forgotten, by multiplication with 0, or kept by multiplication with 1 (or values close to either). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 LSTM - step by step\n",
    "\n",
    "Note that 2.1 and 2.2 are preparation for the CS update, the actual update happens in 2.3.\n",
    "\n",
    "### 2.1 Forgetting\n",
    "'Forgetting' happens in the first gate. This sigmoid layer decides which information should be kept in the cell state and which inforamtion should be thrown away.\n",
    "\n",
    "<img src =\"LSTM3-focus-f.png\">\n",
    "\n",
    "### 2.2 Adding to the CS\n",
    "\n",
    "This step is consists of two substeps. First the 'input gate' decides which values should be updated and a tanh layer creates of vector of potential values that could be added to the CS. Then their outputs are combined to an update for the CS.\n",
    "\n",
    "<img src =\"LSTM3-focus-i.png\">\n",
    "\n",
    "### 2.3 Update\n",
    "\n",
    "We can now compute the new CS.\n",
    "\n",
    "<img src=\"LSTM3-focus-C.png\">\n",
    "\n",
    "### 2.4 Output\n",
    "\n",
    "Until now we have only worked on changes to the CS, so we still need to compute an actual output. First another sigmoid layer decides which part of the CS is going to be output, while a <em>tanh()</em> is applied on the CS. Both resulting vectors are multiplied to the acutal output vector.\n",
    "\n",
    "<img src=\"LSTM3-focus-o.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Recap And Remarks\n",
    "\n",
    "<img src=\"LSTM3-chain.png\">\n",
    "<img src=\"LSTM2-notation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Dimensions\n",
    "A little technical oddity, which might raise a few questions, is the use of 'concatenation' of the x<sub>t</sub> input and the h<sub>t-1</sub> hidden state vectors (Which is analogous to String concatenation we know from basic proramming). Most obviously this could lead to a problem in the forget branch, where we feed the concatenated vectors into a sigmoid layer and multiply the layers output elementwise with the CS.\\\n",
    "The concatenated vectors have a dimension (or length) of dim(h<sub>t-1</sub>)+dim(x<sub>t</sub>). Since dim(h<sub>t-1</sub>)=dim(CS), the sigmoid layer cannot just assign a [0,1] value to each of its inputs, or else the following multiplication wouldn't work. Which means that the sigmoids output and the CS have to have the same dimension. \\\n",
    "Similar this relation with the CS must hold for layers and thus the output dimensions in the cell must all be the same.\n",
    "\n",
    "A good short example, which also shows how batch size wouldn't have a, can be found [here](https://www.quora.com/Does-the-input-dimension-have-to-match-the-cell-state-or-the-hidden-state-dimension-in-an-LSTM ).\n",
    "\n",
    "### 3.2 CS And Output\n",
    "In the diagrams above, we see that the output h<sub>t</sub> has the same dimension "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
