{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quelle**\\\n",
    "$\\rightarrow$ <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - Long Short Term Memory Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Long Short Term Memory** networks or \"LSTMs\" are a kind of RNN, designed to solve the long-term dependency problem. They were introduced by Hochreiter & Schmidhuber (1997)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a single neural network layer, they consist of 4 special layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LSTM3-chain.png\">\n",
    "<img src=\"LSTM2-notation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Cell State and Gates\n",
    "\n",
    "The cell state (CS) acts as the memory of the LSTM. As the network proceeds to work on different components of the input series, the information can be added to the cell state or removed from it. Thus the LSTM has the ability to control the passed on information.\\\n",
    "Removal and addition of information happens at so called gates. Gates are neural networks that decide which information should be passed on through the cell state. The sigmoid function is a suitable activation function, because it maps to values ranging from 0 to 1. So information can be forgotten, by multiplication with 0, or kept by multiplication with 1 (or values close to either)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 LSTM - step by step\n",
    "\n",
    "Note that 2.1 and 2.2 are preparation for the CS update, the actual update happens in 2.3.\n",
    "\n",
    "### 2.1 Forgetting\n",
    "'Forgetting' happens in the first gate. This sigmoid layer decides which information should be kept in the cell state and which inforamtion should be thrown away.\n",
    "\n",
    "<img src =\"LSTM3-focus-f.png\">\n",
    "\n",
    "### 2.2 Adding to the CS\n",
    "\n",
    "This step is consists of two substeps. First the 'input gate' decides which values should be updated and a tanh layer creates of vector of potential values that could be added to the CS. Then their outputs are combined to an update for the CS.\n",
    "\n",
    "<img src =\"LSTM3-focus-i.png\">\n",
    "\n",
    "### 2.3 Update\n",
    "\n",
    "We can now compute the new CS.\n",
    "\n",
    "<img src=\"LSTM3-focus-C.png\">\n",
    "\n",
    "### 2.4 Output\n",
    "\n",
    "Until now we have only worked on changes to the CS, so we still need to compute an actual output. First another sigmoid layer decides which part of the CS is going to be output, while a <em>tanh()</em> is applied on the CS. Both resulting vectors are multiplied to the acutal output vector.\n",
    "\n",
    "<img src=\"LSTM3-focus-o.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Recap\n",
    "\n",
    "<img src=\"LSTM3-chain.png\">\n",
    "<img src=\"LSTM2-notation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
