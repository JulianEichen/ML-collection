{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Cross Entropy - Kreuzentropie\n",
    "\n",
    "\"Die Kreuzentropie ist in der Informationstheorie und der mathematischen Statistik ein Maß für die Qualität eines Modells für eine Wahrscheinlichkeitsverteilung.\" \n",
    "-[Wiki DE](https://de.wikipedia.org/wiki/Kreuzentropie)\n",
    "\n",
    "\"Kreuzentropie ist ein Maß für den Unterschied zwischen zwei Wahrscheinlichkeitsverteilungen für eine gegebene Zufallsvariable oder eine Menge an Ereignissen.\" -https://machinelearningmastery.com/cross-entropy-for-machine-learning/\n",
    "\n",
    "\"In information theory, the **cross-entropy** between two probability distributions  _p_ and _q_ over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution _q_, rather than the true distribution _p_. \" -[Wiki EN](https://en.wikipedia.org/wiki/Cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grundlagen:\n",
    "### Informationsgehalt\n",
    "\n",
    "Information bzw. der **Informationsgehalt** (oder auch **Überraschungswert**) einer Nachricht ist messbar als die Anzahl an Bits, die benötigt werden um ein (stochastisches) Ereignis zu codieren und zu übermitteln. Hierbei haben Ereignisse mit niedriger Wahrscheinlichkeit einen hohen Informationsgehalt und Ereignisse mit hoher Warscheinlichkeit einen niedrigen Informationsgehalt.\n",
    "\n",
    "Definition nach Claude Shannon:\n",
    "+ Ein Ereignis mit Wahrscheinlichkeit 1 (oder 100%) ist perfekt nicht überraschend und enthält keine Information\n",
    "+ Je unwahrscheinlicher ein Ereignis ist, desto überraschender ist es und umso mehr Informationsgehalt hat es\n",
    "+ Wenn zwei (stochastisch) unabhängige Ereignisse separat gemessen werden, ist die ihr gesamter Informationsgehalt die Summer der jeweiligen Informationsgehalte\n",
    "\n",
    "\n",
    "Es kann gezeigt werden, dass für ein Ereignis $x$ mit Wahrscheinlichkeit $P$, der Überraschungswert (bzw der Informationsgehalt), wie folgt gemessen werden kann:\\\n",
    "$\\mathrm{I}(x) := -\\log_b(\\mathrm{P}(x)) = \\log_b(1/\\mathrm{P}(x))$\n",
    "\n",
    "Wählt man als Basis $b\\!=\\!2$ erhält man den Wert für $\\mathrm{I}$ in der Einheit $\\text{Bit}$ (oder auch $\\text{Shannon}$). Weitere Einheiten sind $\\text{Nat}$ für $b\\!=\\!e$ und $\\text{Hartley}$ für $b\\!=\\!10$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beispiele:\n",
    "### 1. Fairer Münzwurf:\n",
    "* Voraussetzung:\\\n",
    "$\\text{Ereignisse}=\\{ \\text{Kopf}, \\text{Zahl}\\}$\\\n",
    "$ \\mathrm{P}(\\text{Kopf}) = \\mathrm{P}(\\text{Zahl}) = 0.5 = 50\\% = \\tfrac{1}{2}$ \n",
    "* Berechnung der Überraschungswerte (in Bit): \\\n",
    "Ereignis Kopf:\\\n",
    "$ \\mathrm{I}(\\text{Kopf}) = -\\log_2 {\\mathrm{P}(\\text{Kopf})}= -\\log_2\\!{\\tfrac{1}{2}} = 1$ \\\n",
    "Ereignis Zahl:\\\n",
    "$ \\mathrm{I}(\\text{Zahl}) = -\\log_2 {\\mathrm{P}(\\text{Zahl})}= -\\log_2\\!{\\tfrac{1}{2}} =1$\n",
    "\n",
    "### 2. Unfairer Münzwurf:\n",
    "* Voraussetzung:\\\n",
    "$\\text{Ereignisse}=\\{ \\text{Kopf}, \\text{Zahl}\\}$\\\n",
    "$ \\mathrm{P}(\\text{Kopf}) = 0.75 = 75\\% = \\tfrac{3}{4}$\\\n",
    "$ \\mathrm{P}(\\text{Zahl}) = 0.25 = 25\\% = \\tfrac{1}{4}$ \n",
    "* Berechnung der Überraschungswerte (in Bit): \\\n",
    "Ereignis Kopf:\\\n",
    "$ \\mathrm{I}(\\text{Kopf}) = -\\log_2 {\\mathrm{P}(\\text{Kopf})}= -\\log_2\\!{\\tfrac{3}{4}} = 0.4150$ \\\n",
    "Ereignis Zahl:\\\n",
    "$ \\mathrm{I}(\\text{Zahl}) = -\\log_2 !{\\mathrm{P}(\\text{Zahl})}= -\\log_2\\!{\\tfrac{1}{4}} =2$\n",
    "\n",
    "### 3. Fairer 6-Seitiger Würfel:\n",
    "* Voraussetzung:\\\n",
    "$\\text{Ereignisse}=\\{1, 2, 3, 4, 5, 6\\}$\\\n",
    "$\\mathrm{P}(1)=\\mathrm{P}(2)=\\mathrm{P}(3)=\\mathrm{P}(4)=\\mathrm{P}(5)=\\mathrm{P}(6)=0.166...= 16.66\\%=1/6$\n",
    "* Berechnung der Überraschungswerte (in Bit): \\\n",
    "Ereignis W=1:\\\n",
    "$ \\mathrm{I}(1) = -\\log_2\\!{P{(1)}}= -\\log_2\\!{\\tfrac{1}{6}} =2.5849...$\\\n",
    "Ereignis W=2:\\\n",
    "$ \\mathrm{I}(2) = -\\log_2\\!{P{(2)}}= -\\log_2\\!{\\tfrac{1}{6}} =2.5849...$\\\n",
    "Ereignis W=3:\\\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropie\n",
    "Man kann die Überraschungswertes eines Ereignisraumes nun wieder als Ereignisse auffassen.\n",
    "* fairer Münzwurf:\\\n",
    "Voraussetzung:\\\n",
    "$\\text{Ereignisse}=\\{ 1 \\}$\\\n",
    "$ \\mathrm{P}(\\text{1}) = 1 = 100\\% $\n",
    "* unfairer Münzwurf:\\\n",
    "Voraussetzung:\\\n",
    "$\\text{Ereignisse}=\\{ 0.4150, 2 \\}$\\\n",
    "$ \\mathrm{P}(\\text{0.4150}) = 0.75 = 75\\% = \\frac{3}{4} $\\\n",
    "$ \\mathrm{P}(\\text{2}) = 0.25 = 25\\% = \\frac{1}{4} $\\\n",
    "\n",
    "Bildet man den Erwartungswert dieser Ereignisse erhält man die Entropie. D.h. den Wert den diese Werte im Mittel annehmen.\n",
    "* Entropie fairer Münzwurf:\\\n",
    "$\\text{Entropie}=\\mathrm{P}(1)*1=1*1=1$\n",
    "* Entropie unfairer Münzwurf:\\\n",
    "$\\text{Entropie}=\\mathrm{P}(0.4150)*0.4150+\\mathrm{P}(2)*2=0.75*0.4150+0.25*2=0.81125$\n",
    "\n",
    "Hier lässt sich auch ein allgemeines Verhalten der Entropie erkennen. Sie ist kleiner für 'verzerrte' Verteilung, da in diesen Werte mit höherer Wahrscheinlichkeit und damit kleinerer Entropie dominieren. Im Mittel ist eine verzerrte Verteilung damit 'weniger überraschend' als eine ausbalancierte Verteilung. \n",
    "\n",
    "Setzen wir für die Ausprägungen des Überraschungswertes die Formel zur Berechnung dieser ein, so erhalten wir nun die direkte Formel für die Entropie: \n",
    "* Erinnerung unfairer Münzwurf:\\\n",
    "$\\text{Ereignisse}=\\{ \\text{Kopf}, \\text{Zahl}\\}$\\\n",
    "$ \\mathrm{P}(\\text{Kopf}) = 0.75 = \\mathrm{P}(\\text{0.4150}) $\\\n",
    "$ \\mathrm{P}(\\text{Zahl}) = 0.25 = \\mathrm{P}(\\text{2})$\\\n",
    "$ \\mathrm{I}(\\text{Kopf}) = -\\log_2 {\\mathrm{P}(\\text{Kopf})}= 0.4150$ \\\n",
    "$ \\mathrm{I}(\\text{Zahl}) = -\\log_2 {\\mathrm{P}(\\text{Zahl})} =2$\n",
    "\n",
    "\n",
    "* Entropie direkt:\\\n",
    "$ \\begin{align} \n",
    "\\text{Entropie}(\\text{Unfairer Münzwurf}) &= \\mathrm{P}(0.4150)*0.4150+\\mathrm{P}(2)*2 \\\\\n",
    " &= \\mathrm{P}(\\text{Kopf})*I(\\text{Kopf})+\\mathrm{P}(\\text{Zahl})*\\mathrm{I}(\\text{Zahl}) \\\\\n",
    " &= \\mathrm{P}(\\text{Kopf})*-\\log_2{P(\\text{Kopf})}+\\mathrm{P}(\\text{Zahl})*-\\log_2{\\mathrm{P}(\\text{Zahl})} \\\\\n",
    "\\end{align}$\n",
    "\n",
    "* oder allgemein: \\\n",
    "Sei $X$ eine diskrete Zufallsvariable mit möglichen Ereignissen $x_1,...,x_n$, welche mit Wahrscheinlichkeiten $\\mathrm{P}(x_1),...,\\mathrm{P}(x_n)$ auftreten, dann ist die Entropie von $X$ definiert als:\\\n",
    "\\\n",
    "$\\mathrm{H}(X)=-\\sum_{i=1}^n{\\mathrm{P}(x_i)\\log{\\mathrm{P}(x_i)}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kreuzentropie\n",
    "### Idee\n",
    "Angenommen wir haben eine vorliegende Zielwahrscheinlichkeitsverteilung P und eine Approximation dieser Zielverteilung Q. Dann ist die Kreuzentropie von Q nach P die Anzahl zusätzlicher Bits, die benötigt werden um eine Ereignis über Q darzustellen anstatt über P. \\\n",
    "\\\n",
    "Die Kreuzentropie kann (im diskreten Fall)wie folgt berechnet werden:\\\n",
    "\\\n",
    "$\\mathrm{H}(\\mathrm{P},\\mathrm{Q})=-\\sum_{i=1}^n{\\mathrm{P}(x_i)\\log{\\mathrm{Q}(x_i)}}$ \\\n",
    "\\\n",
    "Bemerkung:\n",
    "* Warhscheinlichkeit der Ereignisse unter $\\mathrm{P}$\n",
    "* Überraschungswert $I()=-log()$ unter $\\mathrm{Q}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kreuzentropie als Loss-Funktion\n",
    "\n",
    "Im Allgemeinen soll bei Klassifizierungsprobleme durch den Input von einer oder mehrere Variablen ein Klassenlabel vorhergesagt werden. \n",
    "\n",
    "Die Voraussetzungen für die Anwendung der Kreuzentropie sind hier wie folgt gegeben: \n",
    "* Die Zielverteilung P ist dadurch gegeben, dass jeder Datenpunkt eines Klassifizierungsdatensatzes __einem__ Label mit einer Wahrscheinlichkeit von 1 zugeordnet werden kann und jedem anderen Label mit einer Wahrscheinlichkeit von 0. \n",
    "* Die zu approximierende Verteilung Q ist genau die Aufgabe ein Modells, also die Wahrscheinlichkeit mit der ein Input einem Label zugeordnet werden kann. \n",
    "* Anders ausgedrückt, wird ein Input als Zufallsvariable $X$ aufgefasst und die möglichen Label(klassen) als die zugehörigen Ereignisse.\n",
    "\n",
    "\n",
    "Da die Verteilung P für ein Input bekannt ist mit Wahrscheinlichkeit 1 oder 0, sehen wir dass die keinerlei Überraschung hat bzw. einen Entropiewert von 0.\\\n",
    "\\\n",
    "Bsp: \\\n",
    "Sei $\\mathrm{A}$ ein Input und die Label $\\mathrm{L} = \\{l_1, l_2, l_3, l_4\\}$, mit $\\mathrm{L}(\\mathrm{A})\\!=\\! l_2$ (also $\\mathrm{A}$ gehört zur Klasse 2). \n",
    "\n",
    "Dann sieht die Verteilung P wie folgt aus: \n",
    "\n",
    "$\\mathrm{P}(l_1)=0$ \\\n",
    "$\\mathrm{P}(l_2)=1$ \\\n",
    "$\\mathrm{P}(l_3)=0$ \\\n",
    "$\\mathrm{P}(l_4)=0$ \n",
    "\n",
    "Mit folgender Entropie: \n",
    "\n",
    "$ \\begin{align}\n",
    "\\mathrm{H}(\\mathrm{P}) &= -(\\mathrm{P}(l_1)*log\\mathrm{P}(l_1) \n",
    "                        +\\mathrm{P}(l_2)*log\\mathrm{P}(l_2) \n",
    "                        +\\mathrm{P}(l_3)*log\\mathrm{P}(l_3) \n",
    "                        +\\mathrm{P}(l_4)*log\\mathrm{P}(l_4) ) \\\\\n",
    "                       &= -(0*log\\mathrm{0} \n",
    "                        + 1*log\\mathrm{1}\n",
    "                        + 0*log\\mathrm{0}\n",
    "                        + 0*log\\mathrm{0} \\\\\n",
    "                       &= 0\n",
    "\\end{align}$\n",
    "\n",
    "Ein (schlecht trainiertes) Modell würde zu einem Input den Klassen verschiedenen Wahrscheinlichkeiten ungleich 0 und 1 zuweisen. Damit würde es zum Beispiel folgende Verteilung Q als Approximation an P erzeugen: \n",
    "\n",
    "$\\mathrm{Q}(l_1)=0.075$ \\\n",
    "$\\mathrm{Q}(l_2)=0.6$ \\\n",
    "$\\mathrm{Q}(l_3)=0.2$ \\\n",
    "$\\mathrm{Q}(l_4)=0.125$ \n",
    "\n",
    "Damit würde sich folgende Kreuzentropie ergeben:\n",
    "\n",
    "$ \\begin{align}\n",
    "\\mathrm{H}(\\mathrm{P},\\mathrm{Q}) &= -(\\mathrm{P}(l_1)*log\\mathrm{Q}(l_1) \n",
    "                        +\\mathrm{P}(l_2)*log\\mathrm{Q}(l_2) \n",
    "                        +\\mathrm{P}(l_3)*log\\mathrm{Q}(l_3) \n",
    "                        +\\mathrm{P}(l_4)*log\\mathrm{Q}(l_4) ) \\\\\n",
    "                       &= -(0*log\\mathrm{0.075} \n",
    "                        + 1*log\\mathrm{0.6}\n",
    "                        + 0*log\\mathrm{0.2}\n",
    "                        + 0*log\\mathrm{0.125} \\\\\n",
    "                       &= 0.74\n",
    "\\end{align}$\n",
    "\n",
    "Oder ein etwas besser trainiertes Modell: \n",
    "\n",
    "$\\mathrm{Q}(l_1)=0.03$ \\\n",
    "$\\mathrm{Q}(l_2)=0.87$ \\\n",
    "$\\mathrm{Q}(l_3)=0.09$ \\\n",
    "$\\mathrm{Q}(l_4)=0.01$ \n",
    "\n",
    "Damit würde sich folgende Kreuzentropie ergeben:\n",
    "\n",
    "$ \\begin{align}\n",
    "\\mathrm{H}(\\mathrm{P},\\mathrm{Q}) &= -(\\mathrm{P}(l_1)*log\\mathrm{Q}(l_1) \n",
    "                        +\\mathrm{P}(l_2)*log\\mathrm{Q}(l_2) \n",
    "                        +\\mathrm{P}(l_3)*log\\mathrm{Q}(l_3) \n",
    "                        +\\mathrm{P}(l_4)*log\\mathrm{Q}(l_4) ) \\\\\n",
    "                       &= -(0*log\\mathrm{0.03} \n",
    "                        + 1*log\\mathrm{0.87}\n",
    "                        + 0*log\\mathrm{0.09}\n",
    "                        + 0*log\\mathrm{0.01} \\\\\n",
    "                       &= 0.2\n",
    "\\end{align}$ \n",
    "\n",
    "Man sieht, dass die Kreuzentropie die Performance der Vorhersage in einem einzelnen Skalar ausdrücken kann und sich damit Loss-Function eingnet. Außerdem ist $\\log_b(x)$ diffbar für alle $x>0$ (welche durch die Definition der Entropie aufgefangen werden), weshalb auch Backpropagation möglich ist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Cross-Entropy und Binary Cross-Entropy\n",
    "\n",
    "Das Prinzip der Kreuzentropie kann nun noch weiter auf die genaue Art des Klassifizierungsproblems angepasst werden.\n",
    "\n",
    "### Arten von Klassifizierung\n",
    "\n",
    "Klassifizierungsprobleme lassen sich in zwei Arten unterteilen: \n",
    "\n",
    "* Ein Input wird genau einer Klasse zugeordnet. \n",
    "\n",
    "* Ein Input kann zu mehreren Klassen gehören.  \n",
    "\n",
    "Die Bezeichnungen für die beiden Arten sind im Allgemeinen __nicht__ einheitlich. Manchmal wird zwischen __Multi-Class__ und __Multi-Label__ unterschieden. \\\n",
    "Wichtig sind hier 2 Dinge: \n",
    "* Aus technischer Sicht wird bei der Zuordnung zu genau einer Klasse eine __One-Hot-Kodierung__ für den Zielvektor genutzt, das heißt nur ein Eintrag ist 1 (bzw. positiv), der Rest 0 (bzw. negativ). Bei der Zuordnung zu potentiell mehreren Klassen enthält der Zielvektor eine oder mehrere 1en und sonst 0. \n",
    "* Bei der Zuordnung zu genau einer Klasse sind die Einträge des Ergebnisvektors __NICHT UNABHÄNGIG__ voneinander. Beipotentiell mehreren Klassen sollten die Einträge sogar als explizit unabhängig voneinander angesehen werden. \n",
    "\n",
    "Damit ergibt sich, dass das 'Genau 1 Klasse'-Problem als einzelnes Gesamtproblem aufgefasst wird. Während das 'Mehrere Klassen'-Problem als Sammlung von verschiedenen 'gehört der Input zur Klasse oder nicht' (oder _binären_) Problemen aufgefasst wird.\n",
    "\n",
    "### Loss-Aktivierungsfuktionen\n",
    "\n",
    "Es werden nun nach dem Output Layer und vor der Berechnung der Kreuzentropie __Sigmoid__ oder __Softmax__ auf den Output-Vektor angewandt. \n",
    "\n",
    "$Sigmoid=f(x_i) = \\frac{1}{1 + e^{-x_{i}}}$ \n",
    "\n",
    "$Softmax =f(x)_{i} = \\frac{e^{x_{i}}}{\\sum_{j=0}^{n} e^{s_{j}}}$\n",
    "\n",
    "Hier soll daran erinnert werden, dass Sigmoid auf einem Vektor die einzelnen Einträge unabhängig von einander auf $(0,1)$ abbildet. Softmax bildet sie auch auf $(0,1)$ ab, jedoch unter der Bedingung dass sie in Summe 1 ergeben. Also erzeugt Softmax eine Abhängigkeit zwischen den einzelnen Einträgen, während Sigmoid dies nicht tut. \n",
    "\n",
    "Wenden wir zum Beispiel Softmax an bevor wir die Kreuzentropie berechnen, trainieren wir das Netzwerk dazu einen Wahrscheinlichkeitsverteilung über die verschiedenen Klassen zu berechnen. Diese Verkettung wird dann __Categorical Cross-Entropy Loss__ genannt: \n",
    "\n",
    "$CCE=-\\sum_{i=1}^C{t_i\\log\\!{(softmax(s)_i})}$ \n",
    "\n",
    "Wobei $t$ der Zielvektor, $s$ der Outputvektor __aus__ dem Outputlayer und $C$ die Anzahl der Klassen ist.\n",
    "\n",
    "Dies kann auf Grund der oben beschriebenen technsichen Umsetzung auch noch weiter vereinfacht und um einen Skalierungsfaktor erweitert werden.\n",
    "\n",
    "Die Verkettung mit Sigmoid ist nicht ganz analog, da das Problem, wie oben beschrieben, in Unterprobleme aufgeteilt wird. Man bestimmt hier die Kreuzentropie einzeln für jedes Unterproblem und summiert diese dann zu einem globalen Loss auf. Eine ausführlichere Erklärung findet man zum Beispiel [hier](https://gombru.github.io/2018/05/23/cross_entropy_loss/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quellen: \\\n",
    "https://de.wikipedia.org/wiki/Kreuzentropie \\\n",
    "https://en.wikipedia.org/wiki/Cross_entropy \n",
    "\n",
    "https://en.wikipedia.org/wiki/Information_content \n",
    "\n",
    "https://gombru.github.io/2018/05/23/cross_entropy_loss/ \n",
    "\n",
    "https://machinelearningmastery.com/what-is-information-entropy/ \\\n",
    "https://machinelearningmastery.com/cross-entropy-for-machine-learning/ \\\n",
    "https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
